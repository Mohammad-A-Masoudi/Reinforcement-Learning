import numpy as np
import matplotlib.pyplot as plt
import gym
env = gym.make('Taxi-v1')
training_episode = 100000
plot_points = 100
episodes_per_point = training_episode//plot_points
#benchmark_data = np.zeros((1, plot_points))

class Agent:
    def __init__(self, env , num_steps , gamma , alpha , beta , policy, VFA ):
        self.env = env
        self.num_steps = num_steps
        self.gamma = gamma
        self.alpha = alpha
        self.beta = beta
        self.nS = env.observation_space.n
        self.nA = env.action_space.n
        self.dim = self.nS , self.nA
        self.policy = policy
        self.VFA = VFA
    
    def learning(self):
        self.learn = True
    
    def notlearning(self):
        self.learn = False
            
    
    def train(self, numberofepisodes):
        self.learning()
        episode_rewards = 0
        state = self.env.reset()
        action = self.policy.take_action(FunctionApproximation , state)
        for i in range(numberofepisodes):
            state , action , reward, done = self.take_step(state , action)
            episode_rewards += reward
            if done: break
        return episode_rewards/numberofepisodes
    

    def benchmark(self ,numberofepisodes):   
        self.notlearning()
        episode_rewards = 0
        state = self.env.reset()
        action = self.policy.take_action(FunctionApproximation , state)
        for i in range(numberofepisodes):
            state , action , reward, done = self.take_step(state , action)
            episode_rewards += reward
            if done: break
        return episode_rewards/numberofepisodes
        
            
    def take_step(self, state , action):
        for j in range(num_steps):
            next_state, reward, done, info = self.env.step(action)
            next_action = self.policy.take_action(FunctionApproximation , next_state)
            if self.learn == True:
                features = self.VFA.feature_state_action(state , action)
                next_features = self.VFA.feature_state_action(next_state , next_action)

                value = self.VFA.get_value(features)
                next_value = self.VFA.get_value(next_features)
                delta = reward + self.gamma*next_value - value

                gradient = self.policy.take_gradient(state , action)
                delta_theta = self.alpha * gradient *value
                self.policy.update_weights(delta_theta)

                delta_weight = self.beta* delta *features
                self.VFA.update_weights(delta_weight)
            return next_state , next_action , reward , done
                  
            
class SoftmaxPolicy:
    def __init__(self, tau = 1):
        self.tau = tau
        self.nS = env.observation_space.n
        self.nA = env.action_space.n
        value = 1
        self.we = np.ones((self.nS,self.nA)) * value
        self.weights = self.we.reshape((self.nS*self.nA,1))
      #  self.dimensions = (self.nS, self.nA)

        
    def update_weights(self, delta_theta):
        self.weights += delta_theta

        
    def take_action(self,FunctionApproximation , state):
        values = np.zeros((self.nA,1))
        for action in range(self.nA):
            feature = VFA.feature_state_action(state,action)
            values[action] = np.dot(feature.T , self.weights)
        softmax_values = np.exp(values/self.tau - max(values))
        probability = (softmax_values / sum(softmax_values)).flatten()
        return np.random.choice(range(self.nA), p = probability)
    
    # def greedy_action():
     #   return np.argmax(probability)
        
    def take_gradient(self , VFA , state , action):
        features= VFA.feature_state_action(state , 0)
        for c in range(1 , self.nA):
            features = np.hstack([features , VFA.feature_state_action(state, c)])
        mean_feature = np.mean(features, 1).reshape(-1,1)
        gradient = (features[: , action].reshape(-1 , 1) - mean_feature)/ self.tau
        return gradient
    
class FunctionApproximation:
    def __init__(self):
        self.nS = env.observation_space.n
        self.nA = env.action_space.n
        value = 1
        self.we = np.ones((self.nS,self.nA)) * value
        self.weights = self.we.reshape((self.nS*self.nA,1))
        self.we_S = np.ones((self.nS,1)) * value
    
   # def feature_state(self, state):
     #   feature = np.zeros((nS, 1))
      #  feature[state] = 1
      #  return feature
    
    def feature_state_action(self, state, action):
        feature = np.zeros((self.nS*self.nA, 1))
        feature[state * self.nA + action] = 1 
        return feature
    
    def feature_state(self, state):
        feature = np.zeros((self.nS,1))
        feature[state] = 1
        return feature
    
    
    def update_weights(self , delta_weight):
        self.weights+= delta_weight
    
    def update_weights_S(self, delta_weight_S):
        self.we_S += delta_weight_S
    
    def get_value(self, features):
        return np.dot(features.T, self.weights)
    
    def get_value_S(self, features):
        return np.dot(features.T, self.we_S)
    
  #  def critic_gradient():
   #     return features
class Agent_TD():
    def __init__(self, env, policy, VFA, alpha, beta, lamda = 0, gamma = 1, num_steps = 1000):
        self.env = env
        self.policy = SoftmaxPolicy()
        self.VFA = FunctionApproximation()
        self.alpha = alpha
        self.beta = beta
    #    self.veta = veta #for state_value
        self.gamma = gamma
        self.num_steps = num_steps
        self.nS = env.observation_space.n
        self.nA = env.action_space.n
        self.dim = self.nS , self.nA
        
    def learning(self):
        self.learn = True
    
    def notlearning(self):
        self.learn = False
        
    def train(self, numberofepisodes):
        self.learning()
        episode_rewards = 0
        state = self.env.reset()
        action = self.policy.take_action(self.VFA , state)
        for i in range(numberofepisodes):
            state , action , reward, done = self.take_step(state , action)
            episode_rewards += reward
            if done: break
        return episode_rewards/numberofepisodes
    
    def take_step(self , state , action):
        next_state , reward, done, info = self.env.step(action)
        next_action = self.policy.take_action(self.VFA , next_state)
        if self.learn == True :
            feature_Q = self.VFA.feature_state_action(state , action)
            next_feature_Q = self.VFA.feature_state_action(next_state, next_action)
            feature_S = self.VFA.feature_state(state)
            next_feature_S = self.VFA.feature_state(next_state)
            
            value_Q = self.VFA.get_value(feature_Q)
            next_value_Q = self.VFA.get_value(next_feature_Q)
      #      value_S = self.VFA.get_value_S(feature_S)
    #        next_value_S = self.VFA.get_value_S(next_feature_S)
            delta_Q = reward + self.gamma * next_value_Q - value_Q
        #    delta_S = reward + self.gamma * next_value_S - value_S
            
        #    advantage_critic = value_Q - value_S
            gradient = self.policy.take_gradient(self.VFA , state , action)
            delta_teta = self.alpha*gradient* delta_Q
            self.policy.update_weights(delta_teta)
            
            delta_weight_Q = self.beta * delta_Q * feature_Q
            self.VFA.update_weights(delta_weight_Q)
        #    delta_weight_S = self.veta * delta_S *feature_S
       #     self.VFA.update_weights_S(delta_weight_S)
            
        return next_state, next_action, reward, done
    policy = SoftmaxPolicy()
VFA = FunctionApproximation()
num_steps = 20
gamma = 1
alpha = 0.2
beta = 0.1
veta = 0.4
amin = Agent_TD( env, policy, VFA, alpha, beta, lamda = 0, gamma = 1, num_steps = 1000)
#benchmark_data = amin.benchmark(100)
for d in range(1, plot_points):
    t = amin.train(episodes_per_point)
    print(t)
